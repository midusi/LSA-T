{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"/mnt/data/datasets/LSA-T\")\n",
    "from type_hints import CutData\n",
    "from helpers.utils import flatten\n",
    "\n",
    "\n",
    "def load_samples_csv(path: Path) -> list[Path]:\n",
    "    with (path).open() as samples_f:\n",
    "        samples = map(Path, list(csv.reader(samples_f))[0])\n",
    "    return list(samples)\n",
    "\n",
    "path = Path(\"../data/cuts/\")\n",
    "samples = {\n",
    "    samples_set.name[:-4]: load_samples_csv(path / samples_set.name) for samples_set in path.glob(\"*.csv\")\n",
    "}\n",
    "samples['all'] = samples['train_min_freq_5_threshold_05'] + samples['test_min_freq_5_threshold_05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_min_freq_2_threshold_05\n",
      "Processing test_min_freq_1_threshold_05\n",
      "Processing train_min_freq_2_threshold_05\n",
      "Processing test_min_freq_5_threshold_05\n",
      "Processing train_min_freq_1_threshold_05\n",
      "Processing train_min_freq_5_threshold_05\n",
      "Processing all\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import TypedDict, Iterable, Optional\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class SetStatistics(TypedDict):\n",
    "    '''type of the cuts json data file'''\n",
    "    labels: list[list[str]]\n",
    "    unique_labels: set[str]\n",
    "    vocab: set[str]\n",
    "    singletons: list[str]\n",
    "    labels_wo_sing: list[list[str]]\n",
    "\n",
    "\n",
    "def get_words_for_set(paths: list[Path], inv_chars: list[str]) -> list[list[str]]:\n",
    "    '''Returns list of words for each cut'''\n",
    "    words: list[list[str]] = []\n",
    "    for cut_path in paths:\n",
    "        with cut_path.open() as datafile:\n",
    "            data: CutData = json.load(datafile)\n",
    "        words.append([w for w in clean_word(data['label'], inv_chars, ' ').lower().split(' ')])\n",
    "    return words\n",
    "\n",
    "def clean_word(word: str, chars: list[str], rep: str = '') -> str:\n",
    "    for c in chars:\n",
    "        word = word.replace(c,rep)\n",
    "    return word\n",
    "\n",
    "def get_statistics(paths: list[Path], inv_chars: list[str], log_playlist: Optional[str] = None) -> SetStatistics:\n",
    "    if log_playlist is not None:\n",
    "        print(f\"Processing {log_playlist}\")\n",
    "    labels = get_words_for_set(paths, inv_chars)\n",
    "    unique_labels = set(map(' '.join, labels))\n",
    "    vocab = set(flatten(labels))\n",
    "    words_freq = Counter(flatten(labels))\n",
    "    singletons = [word for word, freq in words_freq.items() if freq == 1]\n",
    "    labels_w_sing = [label for label in labels if any(((word in singletons) for word in label))]\n",
    "    return {\n",
    "        'labels': labels,\n",
    "        'unique_labels': unique_labels,\n",
    "        'vocab': vocab,\n",
    "        'singletons': singletons,\n",
    "        'labels_wo_sing': labels_w_sing\n",
    "    }\n",
    "\n",
    "inv_chars = ['\\n', ',', '.', '\"', '-', '?', '!', '¿', '¡', '_']\n",
    "\n",
    "statistics = {k: get_statistics(v, inv_chars, log_playlist=k) for k,v in samples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>sentences</th>\n",
       "      <th>unique_sentences</th>\n",
       "      <th>unique_sentences_perc</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>singletons</th>\n",
       "      <th>singletons_perc</th>\n",
       "      <th>labels_w_sing_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_min_freq_2_threshold_05</td>\n",
       "      <td>1776</td>\n",
       "      <td>1750</td>\n",
       "      <td>98.536036</td>\n",
       "      <td>3318</td>\n",
       "      <td>1807</td>\n",
       "      <td>54.460518</td>\n",
       "      <td>60.135135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_min_freq_1_threshold_05</td>\n",
       "      <td>2735</td>\n",
       "      <td>2708</td>\n",
       "      <td>99.012797</td>\n",
       "      <td>5546</td>\n",
       "      <td>3433</td>\n",
       "      <td>61.900469</td>\n",
       "      <td>67.970750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_min_freq_2_threshold_05</td>\n",
       "      <td>7226</td>\n",
       "      <td>6853</td>\n",
       "      <td>94.838085</td>\n",
       "      <td>6287</td>\n",
       "      <td>2024</td>\n",
       "      <td>32.193415</td>\n",
       "      <td>22.695821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_min_freq_5_threshold_05</td>\n",
       "      <td>910</td>\n",
       "      <td>895</td>\n",
       "      <td>98.351648</td>\n",
       "      <td>1579</td>\n",
       "      <td>771</td>\n",
       "      <td>48.828372</td>\n",
       "      <td>54.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_min_freq_1_threshold_05</td>\n",
       "      <td>11065</td>\n",
       "      <td>10693</td>\n",
       "      <td>96.638048</td>\n",
       "      <td>12385</td>\n",
       "      <td>6442</td>\n",
       "      <td>52.014534</td>\n",
       "      <td>40.976051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_min_freq_5_threshold_05</td>\n",
       "      <td>3767</td>\n",
       "      <td>3495</td>\n",
       "      <td>92.779400</td>\n",
       "      <td>2694</td>\n",
       "      <td>625</td>\n",
       "      <td>23.199703</td>\n",
       "      <td>14.361561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>4677</td>\n",
       "      <td>4299</td>\n",
       "      <td>91.917896</td>\n",
       "      <td>2826</td>\n",
       "      <td>517</td>\n",
       "      <td>18.294409</td>\n",
       "      <td>9.856746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             set  sentences  unique_sentences  \\\n",
       "0   test_min_freq_2_threshold_05       1776              1750   \n",
       "1   test_min_freq_1_threshold_05       2735              2708   \n",
       "2  train_min_freq_2_threshold_05       7226              6853   \n",
       "3   test_min_freq_5_threshold_05        910               895   \n",
       "4  train_min_freq_1_threshold_05      11065             10693   \n",
       "5  train_min_freq_5_threshold_05       3767              3495   \n",
       "6                            all       4677              4299   \n",
       "\n",
       "   unique_sentences_perc  vocab_size  singletons  singletons_perc  \\\n",
       "0              98.536036        3318        1807        54.460518   \n",
       "1              99.012797        5546        3433        61.900469   \n",
       "2              94.838085        6287        2024        32.193415   \n",
       "3              98.351648        1579         771        48.828372   \n",
       "4              96.638048       12385        6442        52.014534   \n",
       "5              92.779400        2694         625        23.199703   \n",
       "6              91.917896        2826         517        18.294409   \n",
       "\n",
       "   labels_w_sing_perc  \n",
       "0           60.135135  \n",
       "1           67.970750  \n",
       "2           22.695821  \n",
       "3           54.285714  \n",
       "4           40.976051  \n",
       "5           14.361561  \n",
       "6            9.856746  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def format_stats(set_name: str, stats: SetStatistics) -> list[Any]:\n",
    "    return [\n",
    "        set_name,\n",
    "        len(stats['labels']),\n",
    "        len(stats['unique_labels']),\n",
    "        100 * len(stats['unique_labels']) / len(stats['labels']),\n",
    "        len(stats['vocab']),\n",
    "        len(stats['singletons']),\n",
    "        100 * len(stats['singletons']) / len(stats['vocab']),\n",
    "        100 * len(stats['labels_wo_sing']) / len(stats['labels'])\n",
    "    ]\n",
    "\n",
    "pd.DataFrame([format_stats(name, stats) for name, stats in statistics.items()],\n",
    "    columns=[\n",
    "        \"set\",\n",
    "        \"sentences\",\n",
    "        \"unique_sentences\",\n",
    "        \"unique_sentences_perc\",\n",
    "        \"vocab_size\",\n",
    "        \"singletons\",\n",
    "        \"singletons_perc\",\n",
    "        \"labels_w_sing_perc\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.664632078670735\n",
      "59.86745213549337\n"
     ]
    }
   ],
   "source": [
    "def out_of_vocabulary(labels: Iterable[list[str]], vocab: Iterable[str]) -> list[list[str]]:\n",
    "    return [label for label in labels if all(((word in vocab) for word in label))]\n",
    "\n",
    "oov = out_of_vocabulary(statistics['test_samples']['labels'], statistics['train_samples']['vocab'])\n",
    "res_oov = out_of_vocabulary(statistics['res_test_samples']['labels'], statistics['res_train_samples']['vocab'])\n",
    "\n",
    "print(100 * len(oov) / len(statistics['test_samples']['labels']))\n",
    "print(100 * len(res_oov) / len(statistics['res_test_samples']['labels']))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a931f1cec853e25af2ee96364aa23a684a809b6da7e1defdac92b12dff587456"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
