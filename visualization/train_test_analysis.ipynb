{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25313/1204068975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cut_paths\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cut_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from helpers.get_cut_paths import get_cut_paths\n",
    "\n",
    "\n",
    "def load_samples_csv(path: Path) -> list[Path]:\n",
    "    with (path).open() as samples_f:\n",
    "        samples = map(Path, list(csv.reader(samples_f))[0])\n",
    "    return list(samples)\n",
    "\n",
    "path = Path(\"../data/cuts/\")\n",
    "\n",
    "samples = {\n",
    "    'train_samples': load_samples_csv(path / \"train.csv\"),\n",
    "    'test_samples': load_samples_csv(path / \"test.csv\"),\n",
    "    'res_train_samples': load_samples_csv(path / \"train_res.csv\"),\n",
    "    'res_test_samples': load_samples_csv(path / \"test_res.csv\"),\n",
    "}\n",
    "samples['all'] = samples['train_samples'] + samples['test_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_samples\n",
      "Processing test_samples\n",
      "Processing res_train_samples\n",
      "Processing res_test_samples\n",
      "Processing all\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import TypedDict, TypeVar, Iterable, Optional\n",
    "from collections import Counter\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "class CutData(TypedDict):\n",
    "    '''type of the cuts json data file'''\n",
    "    label: str\n",
    "    start: float\n",
    "    end: float\n",
    "    video: str\n",
    "    playlist: str\n",
    "\n",
    "class SetStatistics(TypedDict):\n",
    "    '''type of the cuts json data file'''\n",
    "    labels: list[list[str]]\n",
    "    unique_labels: set[str]\n",
    "    vocab: set[str]\n",
    "    singletons: list[str]\n",
    "    labels_wo_sing: list[list[str]]\n",
    "\n",
    "\n",
    "def get_words_for_set(paths: list[Path], inv_chars: list[str]) -> list[list[str]]:\n",
    "    '''Returns list of words for each cut'''\n",
    "    words: list[list[str]] = []\n",
    "    for cut_path in paths:\n",
    "        with cut_path.open() as datafile:\n",
    "            data: CutData = json.load(datafile)\n",
    "        words.append([w for w in clean_word(data['label'], inv_chars, ' ').lower().split(' ')])\n",
    "    return words\n",
    "\n",
    "def clean_word(word: str, chars: list[str], rep: str = '') -> str:\n",
    "    for c in chars:\n",
    "        word = word.replace(c,rep)\n",
    "    return word\n",
    "\n",
    "def flatten(list: Iterable[Iterable[T]]) -> list[T]:\n",
    "    return [item for sublist in list for item in sublist]\n",
    "\n",
    "def get_statistics(paths: list[Path], inv_chars: list[str], log_playlist: Optional[str] = None) -> SetStatistics:\n",
    "    if log_playlist is not None:\n",
    "        print(f\"Processing {log_playlist}\")\n",
    "    labels = get_words_for_set(paths, inv_chars)\n",
    "    unique_labels = set(map(' '.join, labels))\n",
    "    vocab = set(flatten(labels))\n",
    "    words_freq = Counter(flatten(labels))\n",
    "    singletons = [word for word, freq in words_freq.items() if freq == 1]\n",
    "    labels_w_sing = [label for label in labels if any(((word in singletons) for word in label))]\n",
    "    return {\n",
    "        'labels': labels,\n",
    "        'unique_labels': unique_labels,\n",
    "        'vocab': vocab,\n",
    "        'singletons': singletons,\n",
    "        'labels_wo_sing': labels_w_sing\n",
    "    }\n",
    "\n",
    "inv_chars = ['\\n', ',', '.', '\"', '-', '?', '!', '¿', '¡', '_']\n",
    "\n",
    "statistics = {k: get_statistics(v, inv_chars, log_playlist=k) for k,v in samples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>sentences</th>\n",
       "      <th>unique_sentences</th>\n",
       "      <th>unique_sentences_perc</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>singletons</th>\n",
       "      <th>singletons_perc</th>\n",
       "      <th>labels_w_sing_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_samples</td>\n",
       "      <td>11931</td>\n",
       "      <td>11494</td>\n",
       "      <td>96.337273</td>\n",
       "      <td>12702</td>\n",
       "      <td>6560</td>\n",
       "      <td>51.645410</td>\n",
       "      <td>38.957338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_samples</td>\n",
       "      <td>2949</td>\n",
       "      <td>2915</td>\n",
       "      <td>98.847067</td>\n",
       "      <td>5702</td>\n",
       "      <td>3502</td>\n",
       "      <td>61.417047</td>\n",
       "      <td>65.140726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>res_train_samples</td>\n",
       "      <td>10970</td>\n",
       "      <td>10628</td>\n",
       "      <td>96.882407</td>\n",
       "      <td>11978</td>\n",
       "      <td>6234</td>\n",
       "      <td>52.045417</td>\n",
       "      <td>39.936190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>res_test_samples</td>\n",
       "      <td>2716</td>\n",
       "      <td>2692</td>\n",
       "      <td>99.116348</td>\n",
       "      <td>5350</td>\n",
       "      <td>3302</td>\n",
       "      <td>61.719626</td>\n",
       "      <td>65.868925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all</td>\n",
       "      <td>14880</td>\n",
       "      <td>14254</td>\n",
       "      <td>95.793011</td>\n",
       "      <td>14239</td>\n",
       "      <td>7150</td>\n",
       "      <td>50.214200</td>\n",
       "      <td>34.966398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 set  sentences  unique_sentences  unique_sentences_perc  \\\n",
       "0      train_samples      11931             11494              96.337273   \n",
       "1       test_samples       2949              2915              98.847067   \n",
       "2  res_train_samples      10970             10628              96.882407   \n",
       "3   res_test_samples       2716              2692              99.116348   \n",
       "4                all      14880             14254              95.793011   \n",
       "\n",
       "   vocab_size  singletons  singletons_perc  labels_w_sing_perc  \n",
       "0       12702        6560        51.645410           38.957338  \n",
       "1        5702        3502        61.417047           65.140726  \n",
       "2       11978        6234        52.045417           39.936190  \n",
       "3        5350        3302        61.719626           65.868925  \n",
       "4       14239        7150        50.214200           34.966398  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def format_stats(set_name: str, stats: SetStatistics) -> list[Any]:\n",
    "    return [\n",
    "        set_name,\n",
    "        len(stats['labels']),\n",
    "        len(stats['unique_labels']),\n",
    "        100 * len(stats['unique_labels']) / len(stats['labels']),\n",
    "        len(stats['vocab']),\n",
    "        len(stats['singletons']),\n",
    "        100 * len(stats['singletons']) / len(stats['vocab']),\n",
    "        100 * len(stats['labels_wo_sing']) / len(stats['labels'])\n",
    "    ]\n",
    "\n",
    "pd.DataFrame([format_stats(name, stats) for name, stats in statistics.items()],\n",
    "    columns=[\n",
    "        \"set\",\n",
    "        \"sentences\",\n",
    "        \"unique_sentences\",\n",
    "        \"unique_sentences_perc\",\n",
    "        \"vocab_size\",\n",
    "        \"singletons\",\n",
    "        \"singletons_perc\",\n",
    "        \"labels_w_sing_perc\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.664632078670735\n",
      "59.86745213549337\n"
     ]
    }
   ],
   "source": [
    "def out_of_vocabulary(labels: Iterable[list[str]], vocab: Iterable[str]) -> list[list[str]]:\n",
    "    return [label for label in labels if all(((word in vocab) for word in label))]\n",
    "\n",
    "oov = out_of_vocabulary(statistics['test_samples']['labels'], statistics['train_samples']['vocab'])\n",
    "res_oov = out_of_vocabulary(statistics['res_test_samples']['labels'], statistics['res_train_samples']['vocab'])\n",
    "\n",
    "print(100 * len(oov) / len(statistics['test_samples']['labels']))\n",
    "print(100 * len(res_oov) / len(statistics['res_test_samples']['labels']))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a931f1cec853e25af2ee96364aa23a684a809b6da7e1defdac92b12dff587456"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
